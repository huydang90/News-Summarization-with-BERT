{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataMining.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OCzxW3ckZPZj","colab_type":"code","colab":{}},"source":["# Import libraries \n","#!pip install jsonlines\n","import time\n","import os\n","import os.path\n","import json\n","import jsonlines\n","import pickle\n","#!pip install sh\n","from sh import gunzip\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IEd5GrgZt34","colab_type":"code","outputId":"1eb7178e-e975-41b2-b2db-bfbf6a1778c6","executionInfo":{"status":"ok","timestamp":1585780937079,"user_tz":-120,"elapsed":923,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Mount Gdrive\n","\n","#Gdrive: nlpclass90@gmail.com\n","#Pass: Hh@12345\n","\n","#Gdrive: nlpclass94@gmail.com\n","#Pass: nlp12345@\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nRtBSatKZw4I","colab_type":"code","colab":{}},"source":["# Set Working directory\n","os.chdir(\"/content/drive/My Drive/News_Summarization_with_BERT/newsroom_data\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f409OVknZ6bf","colab_type":"code","outputId":"7980ec9c-a49f-4465-9ee0-e47615d29a10","executionInfo":{"status":"ok","timestamp":1585554613407,"user_tz":-120,"elapsed":90022,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# Untar the file 'complete.tar'; run completed, hence the commented command\n","#!tar -xvf complete.tar"],"execution_count":0,"outputs":[{"output_type":"stream","text":["release/\n","release/dev.jsonl.gz\n","release/test.jsonl.gz\n","release/train.jsonl.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SUHZD5bRmMan","colab_type":"code","outputId":"3f525ba8-39d9-4cb1-9f2c-da136c48eca3","executionInfo":{"status":"ok","timestamp":1585653398225,"user_tz":-120,"elapsed":20482,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["# Unzip the .gz files; run completed, hence the commented commands \n","#gunzip('train.jsonl.gz') #extracts .jsonl and deletes .gz\n","#gunzip('dev.jsonl.gz')\n","#gunzip('test.jsonl.gz')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"BnGshi8YgCQS","colab_type":"code","outputId":"ea139844-2348-4d2e-ce16-0a4faf976670","executionInfo":{"status":"ok","timestamp":1585557459605,"user_tz":-120,"elapsed":477,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#create new directory - DONE\n","cwd = os.getcwd()\n","print(cwd)\n","path = os.path.join(cwd, 'raw_data')\n","\n","if not os.path.exists(path):\n","  os.mkdir(path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/News_Summarization_with_BERT/newsroom_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IAtNzvHzWEzR","colab_type":"code","outputId":"c2951736-3b2c-4233-96f1-3865a34424fe","executionInfo":{"status":"ok","timestamp":1585753663960,"user_tz":-120,"elapsed":6501,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["# Change directory to where stanford nlp is located\n","os.chdir(\"/content/drive/My Drive/News_Summarization_with_BERT/newsroom_data\")\n","\n","\n","\n","#unzip uploaded stanford corenlp folder ; runs completed hence, commented codes\n","#from zipfile import ZipFile\n","\n","#with ZipFile('stanford-corenlp-full-2018-10-05.zip', 'r') as zipObj:\n","  # Extract all the contents of zip file in current directory\n","  #zipObj.extractall()\n","\n","# Import the CORENLP module\n","!pip install stanfordnlp\n","\n","from stanfordnlp.server import CoreNLPClient"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting stanfordnlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n","\r\u001b[K     |██                              | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.18.2)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (46.0.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n","Installing collected packages: stanfordnlp\n","Successfully installed stanfordnlp-0.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dKjptZDqaFZ1","colab_type":"code","colab":{}},"source":["# Set the CORENLP_HOME environment variable to the location of the folder\n","os.environ['CORENLP_HOME'] = r\"/content/drive/My Drive/News_Summarization_with_BERT/newsroom_data/stanford-corenlp-full-2018-10-05\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TFVqlDx_Xuoe","colab_type":"code","outputId":"15614807-e90c-4c2b-a678-d127ed58eccb","executionInfo":{"status":"ok","timestamp":1585753680852,"user_tz":-120,"elapsed":637,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n","# 'client' splits the text input into words (tokens), and further splits the sequence of tokens into sentences. The output is returned as json object\n","client = CoreNLPClient(annotators=['tokenize','ssplit'], output_format= 'json', memory='4G', endpoint='http://localhost:9001')\n","print(client)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<stanfordnlp.server.client.CoreNLPClient object at 0x7ffa90b4b828>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pCvMb3rVUkqi","colab_type":"code","colab":{}},"source":["# Some codes from LEAFNATS(https://github.com/tshi04/LeafNATS/blob/master/LeafNATS/tools/newsroom_process/extract_corenlp.py) are borrowed and modified\n","\n","def extract_data(jlfile, n):\n","  \"\"\"Extracts all plain text from .jsonl file and writes article and respective summary text to individual files. \n","  To delineate the summary from the main article, each sentence of the summary is prefixed with <s>.\n","  \n","  Keyword arguments:\n","  jlfile -- a jsonl file; each line in the file is an object representing a single article-summary pair\n","  n -- number of files to be extracted\n","  \"\"\"\n","  sen_array = []\n","\n","  # read the file entry (article) by entry (article)\n","  with jsonlines.open(jlfile) as f: \n","    c = 1\n","    for entry in f:\n","      # print progress at every 5000th file\n","      if(c%5000 == 0): \n","        print(\"%d files processed\"%c)\n","      #print(entry.keys()) #keys are url, archive, title, date, text, summary, density...\n","      article = entry[\"text\"]\n","      summary = entry[\"summary\"]\n","\n","      if article is None or summary is None:\n","        return ''\n","\n","      article = article[:99999] # default max length string processed by corenlp = 100000\n","      # split text input into tokenized sentences containing tokenized words\n","      article = client.annotate(article)\n","      summary = client.annotate(summary)\n","\n","      sen_arr = []\n","      # for every sentence\n","      for sen in article['sentences']:\n","        # return list of tokenized words\n","        sen = [s['originalText'] for s in sen['tokens'] if not '\\n' in s['originalText']]\n","        # collapse the list into a proper sentence, with words separated by space\n","        sen = ' '.join(sen)\n","        # append sentence to the end of list 'sen_array'\n","        sen_arr.append(sen)\n","      # collapse the list into a proper article text, with sentences separated by space\n","      article = ' '.join(sen_arr)\n","    \n","      sen_arr = []\n","      # for every sentence\n","      for sen in summary['sentences']:\n","        # return list of tokenized words\n","        sen = [s['originalText'] for s in sen['tokens'] if not '\\n' in s['originalText']]\n","        # add '<s>' as first element of the list\n","        sen = ['<s>']+sen\n","        # collapse the list into a proper sentence, with words separated by space\n","        sen = ' '.join(sen)\n","        # append sentence to the end of list 'sen_array'\n","        sen_arr.append(sen)\n","      # collapse the list into a proper summary text, with each sentence separated by space, but prefixed by '<s>'\n","      summary = ' '.join(sen_arr)\n","\n","      # collect article and summary into a list\n","      sen_arr = [article, summary]\n","      # separate article text from summary text by new line tab\n","      sen_arr = '\\n'.join(sen_arr)\n","      \n","      if 'train' in jlfile:\n","        # write article-summary text into new training file\n","        with open(\"./raw_data/newsroom_train_%d.p\"%c, \"w\", newline='\\n') as output:\n","          output.write(sen_arr)\n","          sen_array = []\n","      elif 'dev' in jlfile:\n","        # write article-summary text into new validation file\n","        with open(\"./raw_data/newsroom_valid_%d.p\"%c, \"w\", newline='\\n') as output:\n","          output.write(sen_arr)\n","          sen_array = []\n","      elif 'test' in jlfile:\n","        # write article-summary text into new test file\n","        with open(\"./raw_data/newsroom_test_%d.p\"%c, \"w\", newline='\\n') as output:\n","          output.write(sen_arr)\n","          sen_array = []\n","\n","\n","      if c >= n:\n","        print('%d files processed'%c)\n","        break\n","\n","      c+=1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikLaqE9MZXpO","colab_type":"code","outputId":"b39a9367-9e46-4811-ca7e-95c4edad5b36","executionInfo":{"status":"ok","timestamp":1585757126367,"user_tz":-120,"elapsed":3228486,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["# Extract 60000 train, 20000 validation and 20000 test files\n","extract_data('train.jsonl', 60000)\n","extract_data('dev.jsonl', 20000)\n","extract_data('test.jsonl', 20000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Starting server with command: java -Xmx4G -cp /content/drive/My Drive/News_Summarization_with_BERT/newsroom_data/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-80899c6e9171474b.props -preload tokenize,ssplit\n","5000 files processed\n","10000 files processed\n","15000 files processed\n","20000 files processed\n","25000 files processed\n","30000 files processed\n","35000 files processed\n","40000 files processed\n","45000 files processed\n","50000 files processed\n","55000 files processed\n","60000 files processed\n","60000 files processed\n","5000 files processed\n","10000 files processed\n","15000 files processed\n","20000 files processed\n","20000 files processed\n","5000 files processed\n","10000 files processed\n","15000 files processed\n","20000 files processed\n","20000 files processed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RV9vQVUngzsg","colab_type":"code","outputId":"211bf3ab-d284-4509-f63e-ff12382a08e6","executionInfo":{"status":"ok","timestamp":1585780952140,"user_tz":-120,"elapsed":7956,"user":{"displayName":"Hannah Slava","photoUrl":"","userId":"18280469710122652222"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Check number of files in the raw_data directory\n","os.chdir(\"/content/drive/My Drive/News_Summarization_with_BERT/newsroom_data/raw_data\")\n","print(len([name for name in os.listdir('.') if os.path.isfile(name)]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100000\n"],"name":"stdout"}]}]}